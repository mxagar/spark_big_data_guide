{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0db5b3ee-00e3-4d39-bd37-a92e231ac77d",
   "metadata": {},
   "source": [
    "# 3. Machine Learning Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc0aa14-cb87-4022-94bc-37b648662f96",
   "metadata": {},
   "source": [
    "This series is based on the Datacamp course [Introduction to PySpark](https://app.datacamp.com/learn/courses/introduction-to-pyspark). The course has the following chapters:\n",
    "\n",
    "1. Basics: Getting to know PySpark\n",
    "2. Manipulating data\n",
    "3. **Getting started with machine learning pipelines**: The current notebook.\n",
    "\n",
    "In this notebook, basic data processing is perform in form of a pipeline and a logistic regression model is trained with grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e66c001-6e1a-4e8d-b377-33f58ac49d50",
   "metadata": {},
   "source": [
    "**Table of Contents:**\n",
    "\n",
    "- [Setup: Create Session + Upload Data](#Setup:-Create-Session-+-Upload-Data)\n",
    "- [3.1 Introduction to Machine Learning in Spark](#3.1-Introductio-to-Machine-Learning-in-Spark)\n",
    "- [3.2 Data Processing Pipeline](#3.2-Data-Processing-Pipeline)\n",
    "    - Join tables\n",
    "    - Cast Types\n",
    "    - New Features/Columns\n",
    "    - Remove Missing Values\n",
    "    - Encode Categoricals\n",
    "    - Assemble a Vector and Create a Pipeline\n",
    "    - Fit and Transform the Pipeline\n",
    "    - Train/Test Split\n",
    "- [3.3 Model Tuning and Selection](#3.3-Model-Tuning-and-Selection)\n",
    "    - Instantiate Logistic Regression Model\n",
    "    - Instantiate Evaluation Metric\n",
    "    - Instantiate Parameter Grid\n",
    "    - Cross Validation Object\n",
    "    - Fit the Model with Grid Search\n",
    "    - Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb1edfb-07e4-4f5f-b93e-e8b8ee1774ac",
   "metadata": {},
   "source": [
    "## Setup: Create Session + Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f702f89-4dd2-4e68-9564-a8568c0e0e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "433b00a0-67dd-4004-8bf6-f1e97bba537d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x00000241E5888E90>\n"
     ]
    }
   ],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or get a (new) SparkSession: session\n",
    "session = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Print session: our SparkSession\n",
    "print(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "764ab17d-3697-44f3-9574-2fa30ee5ec10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='airports', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True), Table(name='flights', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True), Table(name='planes', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "# Load and register flights dataframe\n",
    "flights = session.read.csv(\"../data/flights_small.csv\", header=True, inferSchema=True)\n",
    "flights.createOrReplaceTempView(\"flights\")\n",
    "\n",
    "# Load and register airports dataframe\n",
    "airports = session.read.csv(\"../data/airports.csv\", header=True, inferSchema=True)\n",
    "airports.createOrReplaceTempView(\"airports\")\n",
    "\n",
    "# Load and register planes dataframe\n",
    "planes = session.read.csv(\"../data/planes.csv\", header=True, inferSchema=True)\n",
    "planes.createOrReplaceTempView(\"planes\")\n",
    "\n",
    "print(session.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b25198-298c-4d42-b017-f7dfdb1c2c17",
   "metadata": {},
   "source": [
    "## 3.1 Introduction to Machine Learning in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae21f49a-2e29-411d-b065-4aae57bd1b71",
   "metadata": {},
   "source": [
    "We have two types of classes defined in the module `pyspark.ml`:\n",
    "\n",
    "- `Transformer` classes: they take a Spark SQL Dataframe and `.transform()` it to yield a new Spark SQL Dataframe.\n",
    "- `Estimator` classes: they take a Spark SQL Dataframe and `.fit()` a model to it to deliver back an object, which can be a trained `Transformer` ready to `transform()` the data. For instance, a model is an `Estimator` which returns a `Transformer`; then, scoring a model consists in calling `transform()` on the returned `Transformer` using the desired dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b5ff27-66bd-44fe-a6d6-fd95664e3a82",
   "metadata": {},
   "source": [
    "## 3.2 Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ab45f9-5e27-49da-b086-a54e44f55976",
   "metadata": {},
   "source": [
    "In this section, basic and typical data processing steps are carried out on the loaded datasets. In spark, feature engineering is done with Pipelines. Shown steps:\n",
    "\n",
    "- Join tables.\n",
    "- Cast types: numeric values are required for modeling.\n",
    "- New Features/Columns\n",
    "- Remove Missing Values\n",
    "- Encode Categoricals\n",
    "- Assemble a Vector and Create a Pipeline\n",
    "- Fit and Transform the Pipeline\n",
    "- Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df61bd91-49a3-4132-9e8c-15545d7b83f0",
   "metadata": {},
   "source": [
    "### Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "445adcae-6e73-4075-827b-cad18d46a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename year column\n",
    "planes = planes.withColumnRenamed(\"year\", \"plane_year\")\n",
    "\n",
    "# Join the DataFrames\n",
    "model_data = flights.join(planes, on=\"tailnum\", how=\"leftouter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec096dd-16bc-42af-892a-4319d48f48dc",
   "metadata": {},
   "source": [
    "### Cast Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "760426c2-ecae-4f72-90e4-e8dd76104fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_delay: string (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: string (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- minute: string (nullable = true)\n",
      " |-- plane_year: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- engines: integer (nullable = true)\n",
      " |-- seats: integer (nullable = true)\n",
      " |-- speed: string (nullable = true)\n",
      " |-- engine: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d3fb19-4e5b-4610-b5c2-3483fc75f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the columns to integers\n",
    "model_data = model_data.withColumn(\"arr_delay\", model_data.arr_delay.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"air_time\", model_data.air_time.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"month\", model_data.month.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"plane_year\", model_data.plane_year.cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70dc771f-a713-4e8c-bfe6-71d0f2c06024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_delay: string (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_delay: integer (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- minute: string (nullable = true)\n",
      " |-- plane_year: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- engines: integer (nullable = true)\n",
      " |-- seats: integer (nullable = true)\n",
      " |-- speed: string (nullable = true)\n",
      " |-- engine: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ae7db8-bef5-41c9-85ea-33edbbc2e664",
   "metadata": {},
   "source": [
    "### New Features/Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e2ba4e2-99cc-4c84-8d1c-40f88cd6a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the column plane_age\n",
    "model_data = model_data.withColumn(\"plane_age\", model_data.year - model_data.plane_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c3c500e-42a2-4a9b-9938-243f4c0eaaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create is_late\n",
    "model_data = model_data.withColumn(\"is_late\", model_data.arr_delay > 0)\n",
    "\n",
    "# Convert to an integer: booleans need to be converted to integers, too\n",
    "model_data = model_data.withColumn(\"label\", model_data.is_late.cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd8be6c-ff13-45f1-a28d-a0bd174db435",
   "metadata": {},
   "source": [
    "### Remove Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29ab0b84-feb6-4347-902d-8dbb8ba26557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values\n",
    "model_data = model_data.filter(\"\"\"arr_delay is not NULL \n",
    "                                  and dep_delay is not NULL\n",
    "                                  and air_time is not NULL\n",
    "                                  and plane_year is not NULL\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b85f1db-6929-43f2-a05d-7f5a5a3f3631",
   "metadata": {},
   "source": [
    "### Encode Categoricals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2abbf37-42a8-4621-aa0b-507d3bd54d2b",
   "metadata": {},
   "source": [
    "We need to instantiate `StringIndexer` to map all unique categorical levels to numbers and a `OneHotEncoder` to create dummy variables from the numbers. All these objects need to be insstantiated and arranged in a vector which is then `fit()` on the dataframe. After that, we can `transform()` the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "007e0a2e-d40a-4b8d-9a72-412d218f1f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fe8cad0-5697-4740-ab70-e3330f5ad6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StringIndexer: Estimator that needs to be fit() and returns a Transformer\n",
    "# StringIndexer: map all unique categorical levels to numbers\n",
    "carr_indexer = StringIndexer(inputCol=\"carrier\",\n",
    "                             outputCol=\"carrier_index\")\n",
    "\n",
    "# Create a OneHotEncoder: Estimator that needs to be fit() and returns a Transformer\n",
    "carr_encoder = OneHotEncoder(inputCol=\"carrier_index\",\n",
    "                             outputCol=\"carrier_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "005513e7-39fd-44de-bf9a-19425589a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StringIndexer: Estimator that needs to be fit() and returns a Transformer\n",
    "# StringIndexer: map all unique categorical levels to numbers\n",
    "dest_indexer = StringIndexer(inputCol=\"dest\",\n",
    "                             outputCol=\"dest_index\")\n",
    "\n",
    "# Create a OneHotEncoder: Estimator that needs to be fit() and returns a Transformer\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_index\",\n",
    "                             outputCol=\"dest_fact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082701af-9f0c-4e3a-9763-de89924c436a",
   "metadata": {},
   "source": [
    "### Assemble a Vector and Create a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69e0156c-3b6e-42fc-8913-79b41a1c1056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a VectorAssembler: Transformer\n",
    "vec_assembler = VectorAssembler(inputCols=[\"month\",\n",
    "                                           \"air_time\",\n",
    "                                           \"carrier_fact\",\n",
    "                                           \"dest_fact\",\n",
    "                                           \"plane_age\"],\n",
    "                                outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c597ae96-f9c4-44a2-90e1-6a68382a963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the pipeline: we append in series all the Estimator/Transformer objects\n",
    "# and the VectorAssembler\n",
    "flights_pipe = Pipeline(stages=[dest_indexer,\n",
    "                                dest_encoder,\n",
    "                                carr_indexer,\n",
    "                                carr_encoder,\n",
    "                                vec_assembler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5521f0-f4f6-4eff-8037-a181d11733af",
   "metadata": {},
   "source": [
    "### Fit and Transform the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72fc1295-f5a3-470d-b9b6-1c0299eab248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the data:\n",
    "# - first, the Estimators are fit, which generate trained Transformers\n",
    "# - then, the dataset is passed through the trained Transformers\n",
    "piped_data = flights_pipe.fit(model_data).transform(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "631fe114-6ce6-4d00-8334-3a2fc6ed99f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_delay: string (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_delay: integer (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- minute: string (nullable = true)\n",
      " |-- plane_year: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- engines: integer (nullable = true)\n",
      " |-- seats: integer (nullable = true)\n",
      " |-- speed: string (nullable = true)\n",
      " |-- engine: string (nullable = true)\n",
      " |-- plane_age: integer (nullable = true)\n",
      " |-- is_late: boolean (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- dest_index: double (nullable = false)\n",
      " |-- dest_fact: vector (nullable = true)\n",
      " |-- carrier_index: double (nullable = false)\n",
      " |-- carrier_fact: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "piped_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975c5fb-007a-4065-9d77-64ac6b98a4f2",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb290757-676c-4513-8dfb-4a0fa3a7f88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "# train 60%, test 40%\n",
    "# Always split after the complete dataset has been processed!\n",
    "training, test = piped_data.randomSplit([.6, .4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c828598b-33e5-4a86-983d-abd7fd9f1fe7",
   "metadata": {},
   "source": [
    "## 3.3 Model Tuning and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a02cffe-f5ef-4f4f-85ac-7ae991d125fe",
   "metadata": {},
   "source": [
    "In this section, a logistic regression model is tuned and trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5166d4-2a90-4ac2-b713-a303602ac0fc",
   "metadata": {},
   "source": [
    "### Instantiate Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90aad538-7ea4-4f6f-97ac-5ccd2afc6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LogisticRegression: Estimator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a LogisticRegression Estimator\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a1cc4-3488-4605-a3af-14f8c43a87bb",
   "metadata": {},
   "source": [
    "### Instantiate Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8bcfb79-856c-46c6-8de6-dabb003a87f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluation submodule\n",
    "import pyspark.ml.evaluation as evals\n",
    "\n",
    "# Create a BinaryClassificationEvaluator\n",
    "evaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5721d1-5785-4a38-9355-76b02da585aa",
   "metadata": {},
   "source": [
    "### Instantiate Parameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c207c66c-3f8a-4239-a8fa-642ac0b15eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tuning submodule\n",
    "import numpy as np\n",
    "import pyspark.ml.tuning as tune\n",
    "\n",
    "# Create the parameter grid\n",
    "grid = tune.ParamGridBuilder()\n",
    "\n",
    "# Add the hyperparameters to be tried in the grid\n",
    "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
    "grid = grid.addGrid(lr.elasticNetParam, [0, 1])\n",
    "\n",
    "# Build the grid\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df40d4e-e030-4e30-96b3-65ded8b1593d",
   "metadata": {},
   "source": [
    "### Cross Validation Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc9bf92c-0665-4f43-9d29-618603fd5e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CrossValidator\n",
    "cv = tune.CrossValidator(estimator=lr,\n",
    "                         estimatorParamMaps=grid,\n",
    "                         evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eeac7b-9531-4ad2-9827-a422956a3cd5",
   "metadata": {},
   "source": [
    "### Fit the Model with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4facf06-42dc-44e3-a8b6-910934048197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit cross validation models\n",
    "models = cv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce80eeed-5092-4acf-a7da-50ab44d4722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best model\n",
    "best_lr = models.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c217d07-a7b9-43f8-b549-99d57963c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also train the model\n",
    "# without cross validation and grid search\n",
    "not_best_lr = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7877d026-9d86-4056-8300-15c8a87d82d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel: uid=LogisticRegression_2acefe7002fa, numClasses=2, numFeatures=81\n"
     ]
    }
   ],
   "source": [
    "# Print best_lr\n",
    "print(best_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95da24f-a518-44e0-b52b-397cf42ea2a8",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8583d265-5ef3-4b83-868f-6cc98a68d0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6890592636741706\n"
     ]
    }
   ],
   "source": [
    "# Use the model to predict the test set\n",
    "# Note that the model does not have a predict() function\n",
    "# but it transforms() the data into predictions!\n",
    "test_results = best_lr.transform(test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "print(evaluator.evaluate(test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e2c36-93a6-4856-8538-550141875682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
