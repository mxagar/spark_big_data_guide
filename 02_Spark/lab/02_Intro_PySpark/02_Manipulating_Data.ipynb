{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea7952e-33ce-4364-bfd6-9ce25cd299f5",
   "metadata": {},
   "source": [
    "# 2. Manipulating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39481d5-9253-4769-bf68-cf8baecf74f3",
   "metadata": {},
   "source": [
    "This series is based on the Datacamp course [Introduction to PySpark](https://app.datacamp.com/learn/courses/introduction-to-pyspark). The course has the following chapters:\n",
    "\n",
    "1. Basics: Getting to know PySpark\n",
    "2. **Manipulating data**: The current notebook.\n",
    "3. Getting started with machine learning pipelines\n",
    "\n",
    "This notebook deals with methods for Spark SQL dataframe manipulation. Many of these methods have an equivalent SQL operator. check my [SQL guide](https://github.com/mxagar/sql_guide) if you need a refresher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8c2e83-f67e-436a-a46b-cb480ab97ccb",
   "metadata": {},
   "source": [
    "**Table of Contents**:\n",
    "\n",
    "- [Setup: Create Session + Upload Data](#Setup:-Create-Session-+-Upload-Data)\n",
    "- [2.1 Creating, Renaming, and Casting Columns](#2.1-Creating,-Renaming,-and-Casting-Columns)\n",
    "- [2.2 SQL in a Nutshell](#2.2-SQL-in-a-Nutshell)\n",
    "- [2.3 Filtering Data: WHERE - filter()](#2.3-Filtering-Data:-WHERE---filter())\n",
    "- [2.4 Selecting Columns: SELECT - select(), selectExpr(), alias()](#2.4-Selecting-Columns:-SELECT---select(),-selectExpr(),-alias())\n",
    "- [2.5 Grouping and Aggregating: GROUP BY, MIN, MAX, COUNT, SUM, AVG, AGG](#2.5-Grouping-and-Aggregating:-GROUP-BY,-MIN,-MAX,-COUNT,-SUM,-AVG,-AGG)\n",
    "    - [Aggregation Functions](#Aggregation-Functions)\n",
    "- [2.6 Joining: JOIN - join()](#2.6-Joining:-JOIN---join())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca3cbb2-b630-48ae-b315-ef917e1e6a14",
   "metadata": {},
   "source": [
    "## Setup: Create Session + Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d6daa65-a7e5-4495-a219-3034c7d2c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f62b69-c2a7-427a-b8be-c3c74aafea62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/21 13:31:08 WARN Utils: Your hostname, kasiopeia.local resolves to a loopback address: 127.0.0.1; using 192.168.1.34 instead (on interface en0)\n",
      "23/04/21 13:31:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/21 13:31:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fde48026e50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mxagar/opt/anaconda3/envs/ds/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or get a (new) SparkSession: session\n",
    "session = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Print session: our SparkSession\n",
    "print(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84027568-9a8a-49e6-8775-c3795447db68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='airports', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True), Table(name='flights', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True), Table(name='planes', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "# Load and register flights dataframe\n",
    "flights = session.read.csv(\"../data/flights_small.csv\", header=True, inferSchema=True)\n",
    "flights.createOrReplaceTempView(\"flights\")\n",
    "\n",
    "# Load and register airports dataframe\n",
    "airports = session.read.csv(\"../data/airports.csv\", header=True, inferSchema=True)\n",
    "airports.createOrReplaceTempView(\"airports\")\n",
    "\n",
    "# Load and register planes dataframe\n",
    "planes = session.read.csv(\"../data/planes.csv\", header=True, inferSchema=True)\n",
    "planes.createOrReplaceTempView(\"planes\")\n",
    "\n",
    "print(session.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f1f9d-202c-4c45-9209-925dd9143996",
   "metadata": {},
   "source": [
    "## 2.1 Creating, Renaming, and Casting Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed465ea-cb97-482e-a716-aa7cd8be14ad",
   "metadata": {},
   "source": [
    "Once we get the Spark SQL dataframe, we can add a new column to it with\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"newCol\", df.oldCol + 1)\n",
    "```\n",
    "\n",
    "However, Spark SQL dataframes are inmutable, i.e., we are creating a new dataframe. Notes:\n",
    "\n",
    "- `df.colName` is a `Column` object, which comes up often. We can also convert a column name string into a `Column` with `pyspark.sql.functions.col`.\n",
    "- `withColumn()` returns the **entire table/dataframe** with a new column. If we want to change a column content, we need to write `\"oldCol\"` instead of `\"newCol\"` in the first argument. We can use it to rename columns, too. The second argment **must** be a `Column` object, created as `df.colName` or `col(\"colName\")`.\n",
    "\n",
    "It might happen that we need to cast the type of a column; to check the types we use `printSchema()` and to cast \n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Print schema with types\n",
    "df.printSchema()\n",
    "\n",
    "# Cast from string to double: new table is created\n",
    "df = df.withColumn(\"air_time\", col(\"air_time\").cast(\"double\"))\n",
    "df = df.withColumn(\"air_time\", df.arr_delay.cast(\"double\"))\n",
    "\n",
    "# Rename column \"air_time\" -> \"flight_duration\"\n",
    "# BUT, the old column is still there if it has another name;\n",
    "# we can drop it using .select(), as shown below\n",
    "df = df.withColumn(\"flight_duration\", flights.air_time)\n",
    "\n",
    "# Another way to rename column names:\n",
    "# this function allows to use two column name strings\n",
    "# AND replaces the previous column\n",
    "df = df.withColumnRenamed(\"flight_duration\", \"air_time\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17f87742-f785-436b-a8f8-91ab700954a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/get the DataFrame flights\n",
    "flights = session.table(\"flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d01612fc-66e4-4f19-b9a6-e5c399a0a1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the head\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d750e19b-784b-4e31-b201-651016008188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column: duration_hrs\n",
    "# General syntax: df = df.withColumn(\"newCol\", df.oldCol + 1)\n",
    "# A new dataframe is returned! That's because dataframes and their columns are inmutable\n",
    "# To modify a colum: df = df.withColumn(\"col\", df.col + 1)\n",
    "# BUT: in reality, we create a new dataframe with the modified column\n",
    "flights = flights.withColumn(\"duration_hrs\", flights.air_time/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27cd22d0-3230-409b-b198-33d3d4b0b174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|      duration_hrs|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|               2.2|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|               6.0|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|              1.85|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|1.3833333333333333|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|2.1166666666666667|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the head\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05cabcd9-c350-4b7b-9608-4f9fe1fed9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_delay: string (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: string (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- minute: string (nullable = true)\n",
      " |-- duration_hrs: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bcc9e23-6ca7-4130-bcd2-a25f4d42b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert air_time and dep_delay (strings) to double to use math operations on them\n",
    "flights = flights.withColumn(\"air_time\", col(\"air_time\").cast(\"double\"))\n",
    "flights = flights.withColumn(\"dep_delay\", col(\"dep_delay\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4da1f61-44d5-4f85-bbc3-709f4177e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column and keep old\n",
    "flights = flights.withColumn(\"flight_duration\", flights.air_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2aae56e6-1d9d-437c-8ec7-c6c1c3cba9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to rename column names\n",
    "# This option replaces the old column\n",
    "# flights = flights.withColumnRenamed(\"flight_duration\", \"air_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ebbe7e9-5c3c-4fd5-9348-80013bb99178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_delay: double (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: double (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- minute: string (nullable = true)\n",
      " |-- duration_hrs: double (nullable = true)\n",
      " |-- flight_duration: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9aab1c-2605-4ca2-b75c-8cccace88ba3",
   "metadata": {},
   "source": [
    "## 2.2 SQL in a Nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3168d-e541-4495-8241-d9e993e8a100",
   "metadata": {},
   "source": [
    "Many Spark SQL Dataframe methods have an equivalent SQL operation.\n",
    "\n",
    "Most common SQL operators: `SELECT`, `FROM`, `WHERE`, `AS`, `GROUP BY`, `COUNT()`, `AVG()`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2be1798-1226-41b7-9d83-f59a94391a6d",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- Get all the contents from the table my_table: we get a table\n",
    "SELECT * FROM my_table;\n",
    "\n",
    "-- Get specified columns and compute a new column value: we get a table\n",
    "SELECT origin, dest, air_time / 60 FROM flights;\n",
    "\n",
    "-- Filter according to value in column: we get a table\n",
    "SELECT * FROM students\n",
    "WHERE grade = 'A';\n",
    "\n",
    "-- Get the table which contains the destination and tail number of flights that last +10h\n",
    "SELECT dest, tail_num FROM flights WHERE air_time > 600;\n",
    "\n",
    "-- Group by: group by category values and apply an aggregation function for each group\n",
    "-- In this case: number of flights for each unique origin\n",
    "SELECT COUNT(*) FROM flights\n",
    "GROUP BY origin;\n",
    "\n",
    "-- Group by all unique combinations of origin and dest columns\n",
    "SELECT origin, dest, COUNT(*) FROM flights\n",
    "GROUP BY origin, dest;\n",
    "\n",
    "-- Group by unique origin-carrier combinations and for each\n",
    "-- compute average air time in hrs\n",
    "SELECT AVG(air_time) / 60 FROM flights\n",
    "GROUP BY origin, carrier;\n",
    "\n",
    "-- Flight duration in hrs, new column name\n",
    "SELECT air_time / 60 AS duration_hrs\n",
    "FROM flights\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcef8ed-02a6-4388-9c2d-6dc018f74bf4",
   "metadata": {},
   "source": [
    "Also, recall we can combine tables in SQL using the `JOIN` operator:\n",
    "\n",
    "```sql\n",
    "-- INNER JOIN: note it is symmetrical, we can interchange TableA and B\n",
    "SELECT * FROM TableA\n",
    "INNER JOIN TableB\n",
    "ON TableA.col_match = TableB.col_match;\n",
    "\n",
    "-- FULL OUTER JOIN\n",
    "SELECT * FROM TableA\n",
    "FULL OUTER JOIN TableB\n",
    "ON TableA.col_match = Table_B.col_match\n",
    "\n",
    "-- LEFT OUTER JOIN\n",
    "-- Left table: TableA; Right table: TableB\n",
    "SELECT * FROM TableA\n",
    "LEFT OUTER JOIN TableB\n",
    "ON TableA.col_match = Table_B.col_match\n",
    "\n",
    "-- RIGHT OUTER JOIN\n",
    "-- Left table: TableA; Right table: TableB\n",
    "SELECT * FROM TableA\n",
    "RIGHT OUTER JOIN TableB\n",
    "ON TableA.col_match = Table_B.col_match\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d13440-8d97-4f89-8793-f9481b5c649a",
   "metadata": {},
   "source": [
    "## 2.3 Filtering Data: WHERE - filter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3323b248-572b-4fe0-99bf-d94372802ead",
   "metadata": {},
   "source": [
    "The `filter()` function is equivalent to `WHERE`. We can pass either a string we would write after `WHERE` or we can use the typical Python syntax:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e3764-66fb-4693-b72a-42392f56ad12",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT * FROM flights WHERE air_time > 120\n",
    "```\n",
    "\n",
    "```python\n",
    "flights.filter(\"air_time > 120\").show()\n",
    "flights.filter(flights.air_time > 120).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0402de48-382c-41a5-9339-13417bf6bfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+---------------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|duration_hrs|flight_duration|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+---------------+\n",
      "|2014|    1| 22|    1040|      5.0|    1505|        5|     AS| N559AS|   851|   SEA| HNL|   360.0|    2677|  10|    40|         6.0|          360.0|\n",
      "|2014|    4| 19|    1236|     -4.0|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|   135.0|    1050|  12|    36|        2.25|          135.0|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+---------------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|duration_hrs|flight_duration|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+---------------+\n",
      "|2014|    1| 22|    1040|      5.0|    1505|        5|     AS| N559AS|   851|   SEA| HNL|   360.0|    2677|  10|    40|         6.0|          360.0|\n",
      "|2014|    4| 19|    1236|     -4.0|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|   135.0|    1050|  12|    36|        2.25|          135.0|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter flights by passing a string\n",
    "long_flights1 = flights.filter(\"distance > 1000\")\n",
    "\n",
    "# Filter flights by passing a column of boolean values\n",
    "long_flights2 = flights.filter(flights.distance > 1000)\n",
    "\n",
    "# Print the data to check they're equal\n",
    "long_flights1.show(2)\n",
    "long_flights2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72071403-d93d-4d3e-9b48-66d1323b6107",
   "metadata": {},
   "source": [
    "## 2.4 Selecting Columns: SELECT - select(), selectExpr(), alias()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcd7606-c78c-4261-afc7-006416fd536e",
   "metadata": {},
   "source": [
    "The `select()` method is equivalent to the `SELECT` SQL operator: it can take several comma-separated column names or `Column` objects (`df.col`) and returns a table with them. In contrast, the `withColumn()` method returns the entire table. Therefore, if we want to drop unnecessary columns, we can use `select()`.\n",
    "\n",
    "If we want to perform a more sophisticated selection, as in SQL, we can use `selectExpr()` and pass comma-separated SQL strings; if we want to change the name of the selected/transformed column, we can use `alias()`, equivalent to `AS` in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5811a1c-0a6a-474f-8542-5c1cecb72308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first set of columns\n",
    "selected1 = flights.select(\"tailnum\", \"origin\", \"dest\")\n",
    "\n",
    "# Select the second set of columns\n",
    "temp = flights.select(flights.origin, flights.dest, flights.carrier)\n",
    "\n",
    "# Define first filter\n",
    "filterA = flights.origin == \"SEA\"\n",
    "\n",
    "# Define second filter\n",
    "filterB = flights.dest == \"PDX\"\n",
    "\n",
    "# Filter the data, first by filterA then by filterB\n",
    "selected2 = temp.filter(filterA).filter(filterB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b81a5a19-e9c4-4793-a98d-0b2bfff927a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define avg_speed\n",
    "# We define a new column object\n",
    "avg_speed = (flights.distance/(flights.air_time/60)).alias(\"avg_speed\")\n",
    "\n",
    "# Select the correct columns\n",
    "# We can pass comma separated strings or column objects to select();\n",
    "# each column is a comma-separated element.\n",
    "speed1 = flights.select(\"origin\", \"dest\", \"tailnum\", avg_speed)\n",
    "\n",
    "# Create the same table using a SQL expression\n",
    "# We can pass comma separated SQL strings to selectExpr(); each colum operation is\n",
    "# a comma-separated element.\n",
    "speed2 = flights.selectExpr(\"origin\", \"dest\", \"tailnum\", \"distance/(air_time/60) as avg_speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f230a9-766b-415a-9e08-2e8cffaa1535",
   "metadata": {},
   "source": [
    "## 2.5 Grouping and Aggregating: GROUP BY, MIN, MAX, COUNT, SUM, AVG, AGG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e006f4a1-5745-41d0-b35a-ec8de7be9d7c",
   "metadata": {},
   "source": [
    "Similarly as it is done in SQL, we can create a `pyspark.sql.GroupedData` object with `groupBy()` and then apply aggregation functions like `min()`, `max()`, `count()`, `sum()`, `avg()`, `agg()`, etc. Note that we can use `groupBy()` in two ways:\n",
    "\n",
    "- If we pass one or more column names to `groupBy()`, i.e., `groupBy(\"col\")`, it will group the table in the classes/unique values of the passed column(s); then, we apply an aggregation function on those groups. This is equivalent to SQL.\n",
    "- If we don't pass a column name to `groupBy()`, each row is a group. This seems not to be useful, but it is in practice, because thanks to it we can apply aggregation functions on the rows that would not be possible otherwise; e.g., `df.min(\"col\")` is not possible, but we need to do `df.groupBy().min(\"col\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e5aa0db-f42c-4883-be23-3bb7b42244e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|dest|min(distance)|\n",
      "+----+-------------+\n",
      "| MSY|         2086|\n",
      "| GEG|          224|\n",
      "| BUR|          817|\n",
      "| SNA|          859|\n",
      "| EUG|          106|\n",
      "+----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group all flights by destination and for them\n",
    "# pick the minimum distance\n",
    "flights.groupBy(\"dest\").min(\"distance\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "949277b6-05a1-4ea4-9bcd-fbd5a4da58d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|min(distance)|\n",
      "+-------------+\n",
      "|          106|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the shortest flight from PDX in terms of distance\n",
    "# Note that in this case we don't pass a column to groupBy, but \n",
    "# concatenate an aggregation function with the column, which applies to all rows\n",
    "flights.filter(flights.origin == \"PDX\").groupBy().min(\"distance\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51cc344d-9962-4a4d-bb7a-d918bf055737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|max(air_time)|\n",
      "+-------------+\n",
      "|        409.0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the longest flight from SEA in terms of air time\n",
    "# Note that in this case we don't pass a column to groupBy, but \n",
    "# concatenate an aggregation function with the column, which applies to all rows\n",
    "flights.filter(flights.origin == \"SEA\").groupBy().max(\"air_time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02095db5-562d-4c29-a44a-3523bea7f83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     avg(air_time)|\n",
      "+------------------+\n",
      "|188.20689655172413|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "| sum(duration_hrs)|\n",
      "+------------------+\n",
      "|25289.600000000126|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average duration of Delta flights\n",
    "# Note that in this case we don't pass a column to groupBy, but \n",
    "# concatenate an aggregation function with the column, which applies to all rows\n",
    "flights.filter(flights.carrier == \"DL\").filter(flights.origin == \"SEA\").groupBy().avg(\"air_time\").show()\n",
    "\n",
    "# Total hours in the air\n",
    "# Note that in this case we don't pass a column to groupBy, but \n",
    "# concatenate an aggregation function with the column, which applies to all rows\n",
    "flights.withColumn(\"duration_hrs\", flights.air_time/60).groupBy().sum(\"duration_hrs\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ac10329-ce35-4009-b39c-b230e73b53f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|tailnum|count|\n",
      "+-------+-----+\n",
      "| N442AS|   38|\n",
      "| N102UW|    2|\n",
      "| N36472|    4|\n",
      "| N38451|    4|\n",
      "| N73283|    4|\n",
      "+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+------------------+\n",
      "|origin|     avg(air_time)|\n",
      "+------+------------------+\n",
      "|   SEA| 160.4361496051259|\n",
      "|   PDX|137.11543248288737|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by tailnum\n",
    "by_plane = flights.groupBy(\"tailnum\")\n",
    "\n",
    "# Number of flights each plane made\n",
    "by_plane.count().show(5)\n",
    "\n",
    "# Group by origin\n",
    "by_origin = flights.groupBy(\"origin\")\n",
    "\n",
    "# Average duration of flights from PDX and SEA\n",
    "by_origin.avg(\"air_time\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990eaaaf-8102-4e41-8896-6a45b132707e",
   "metadata": {},
   "source": [
    "### Aggregation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a5a006-ac50-4bb2-8957-51a0560e4b74",
   "metadata": {},
   "source": [
    "The module `pyspark.sql.functions` contains many aggregation functions which we can use with the generic `agg()` method which is applied after any `groupBy()`:\n",
    "\n",
    "- `abs()`: Computes the absolute value of a column.\n",
    "- `avg()`: Computes the average of a column.\n",
    "- `stddev()`: Computes the standard deviation of a column.\n",
    "- `col()`: Returns a column based on the given column name.\n",
    "- `concat()`: Concatenates multiple columns together.\n",
    "- `count()`: Counts the number of non-null values in a column.\n",
    "- `date_format()`: Formats a date or timestamp column based on a specified format string.\n",
    "- `dayofmonth()`: Extracts the day of the month from a date or timestamp column.\n",
    "- `explode()`: Transforms an array column into multiple rows.\n",
    "- `first()`: Returns the first value of a column in a group.\n",
    "- `lit()`: Creates a column with a literal value.\n",
    "- `max()`: Computes the maximum value of a column.\n",
    "- `min()`: Computes the minimum value of a column.\n",
    "- `month()`: Extracts the month from a date or timestamp column.\n",
    "- `round()`: Rounds a column to a specified number of decimal places.\n",
    "- `split()`: Splits a string column based on a delimiter.\n",
    "- `sum()`: Computes the sum of a column.\n",
    "- `to_date()`: Converts a string column to a date column.\n",
    "- `to_timestamp()`: Converts a string column to a timestamp column.\n",
    "- `udf()`: Defines a user-defined function that can be used in PySpark.\n",
    "\n",
    "... and many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14283fd9-6612-4269-a79b-a683974adfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------------------+\n",
      "|month|dest|    avg(dep_delay)|\n",
      "+-----+----+------------------+\n",
      "|    4| PHX|1.6833333333333333|\n",
      "|    1| RDM|            -1.625|\n",
      "|    5| ONT|3.5555555555555554|\n",
      "|    7| OMA|              -6.5|\n",
      "|    8| MDW|              7.45|\n",
      "+-----+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+----+----------------------+\n",
      "|month|dest|stddev_samp(dep_delay)|\n",
      "+-----+----+----------------------+\n",
      "|    4| PHX|    15.003380033491737|\n",
      "|    1| RDM|     8.830749846821778|\n",
      "|    5| ONT|    18.895178691342874|\n",
      "|    7| OMA|    2.1213203435596424|\n",
      "|    8| MDW|    14.467659032985843|\n",
      "+-----+----+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark.sql.functions as F\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Group by month and dest\n",
    "by_month_dest = flights.groupBy(\"month\", \"dest\")\n",
    "\n",
    "# Average departure delay by month and destination\n",
    "by_month_dest.avg(\"dep_delay\").show(5)\n",
    "\n",
    "# Standard deviation of departure delay\n",
    "by_month_dest.agg(F.stddev(\"dep_delay\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687c7a8f-9248-465a-a090-3fab498cb642",
   "metadata": {},
   "source": [
    "## 2.6 Joining: JOIN - join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c644ecc-18ab-4899-a000-63b1e7eabf75",
   "metadata": {},
   "source": [
    "Joining Spark SQL dataframes is very similar to joining in SQL: we combine tables given the index values on key columns.\n",
    "\n",
    "```python\n",
    "dj_joined = df_left.join(other=df_right,\n",
    "                         on=\"key_col\",\n",
    "                         how=\"left_outer\")\n",
    "\n",
    "# Possible values for how: \n",
    "#    \"inner\" (default), \"outer\", \"left_outer\", \"right_outer\", \"leftsemi\", and \"cross\"\n",
    "# Other arguments for join():\n",
    "# - suffixes: tuple of strings to append to the column names that overlap between the two DataFrames.\n",
    "#    By default: \"_x\" and \"_y\"\n",
    "# - broadcast: boolean value indicating whether to broadcast the smaller DataFrame to all nodes in the cluster\n",
    "#    to speed up the join operation. By default: False\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "277c8514-4ca3-4711-b345-1352da4b2680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+-----------+----+---+---+\n",
      "|faa|                name|       lat|        lon| alt| tz|dst|\n",
      "+---+--------------------+----------+-----------+----+---+---+\n",
      "|04G|   Lansdowne Airport|41.1304722|-80.6195833|1044| -5|  A|\n",
      "|06A|Moton Field Munic...|32.4605722|-85.6800278| 264| -5|  A|\n",
      "|06C| Schaumburg Regional|41.9893408|-88.1012428| 801| -6|  A|\n",
      "|06N|     Randall Airport| 41.431912|-74.3915611| 523| -5|  A|\n",
      "|09J|Jekyll Island Air...|31.0744722|-81.4277778|  11| -4|  A|\n",
      "+---+--------------------+----------+-----------+----+---+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Examine the data\n",
    "print(airports.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cc9006a-1f6c-4337-8e52-fd81fc0ab52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the faa column to be dest\n",
    "airports = airports.withColumnRenamed(\"faa\", \"dest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9933874-730b-406c-92eb-a233e00482e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+-----------+----+---+---+\n",
      "|dest|                name|       lat|        lon| alt| tz|dst|\n",
      "+----+--------------------+----------+-----------+----+---+---+\n",
      "| 04G|   Lansdowne Airport|41.1304722|-80.6195833|1044| -5|  A|\n",
      "| 06A|Moton Field Munic...|32.4605722|-85.6800278| 264| -5|  A|\n",
      "| 06C| Schaumburg Regional|41.9893408|-88.1012428| 801| -6|  A|\n",
      "| 06N|     Randall Airport| 41.431912|-74.3915611| 523| -5|  A|\n",
      "| 09J|Jekyll Island Air...|31.0744722|-81.4277778|  11| -4|  A|\n",
      "+----+--------------------+----------+-----------+----+---+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Examine the data\n",
    "print(airports.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39ee41dd-5ba4-4770-86b7-615a81fdba74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+------------------+---------------+--------------------+---------+-----------+---+---+---+\n",
      "|dest|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|air_time|distance|hour|minute|      duration_hrs|flight_duration|                name|      lat|        lon|alt| tz|dst|\n",
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+------------------+---------------+--------------------+---------+-----------+---+---+---+\n",
      "| LAX|2014|   12|  8|     658|     -7.0|     935|       -5|     VX| N846VA|  1780|   SEA|   132.0|     954|   6|    58|               2.2|          132.0|    Los Angeles Intl|33.942536|-118.408075|126| -8|  A|\n",
      "| HNL|2014|    1| 22|    1040|      5.0|    1505|        5|     AS| N559AS|   851|   SEA|   360.0|    2677|  10|    40|               6.0|          360.0|       Honolulu Intl|21.318681|-157.922428| 13|-10|  N|\n",
      "| SFO|2014|    3|  9|    1443|     -2.0|    1652|        2|     VX| N847VA|   755|   SEA|   111.0|     679|  14|    43|              1.85|          111.0|  San Francisco Intl|37.618972|-122.374889| 13| -8|  A|\n",
      "| SJC|2014|    4|  9|    1705|     45.0|    1839|       34|     WN| N360SW|   344|   PDX|    83.0|     569|  17|     5|1.3833333333333333|           83.0|Norman Y Mineta S...|  37.3626|-121.929022| 62| -8|  A|\n",
      "| BUR|2014|    3|  9|     754|     -1.0|    1015|        1|     AS| N612AS|   522|   SEA|   127.0|     937|   7|    54|2.1166666666666667|          127.0|            Bob Hope|34.200667|-118.358667|778| -8|  A|\n",
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+------------------+---------------+--------------------+---------+-----------+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Join the DataFrames\n",
    "flights_with_airports = flights.join(airports,\n",
    "                                     on=\"dest\",\n",
    "                                     how=\"left_outer\")\n",
    "\n",
    "# Examine the new DataFrame\n",
    "print(flights_with_airports.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c5c2a4-14b5-40be-86fc-b23540440d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
