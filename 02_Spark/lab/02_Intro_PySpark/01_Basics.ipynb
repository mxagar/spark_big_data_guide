{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c58074-4bd1-48cd-9036-25066893d725",
   "metadata": {},
   "source": [
    "# 1. Basics: Getting to know PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79237e-3842-4e39-b8fe-2b0bd9f95b93",
   "metadata": {},
   "source": [
    "This series is based on the Datacamp course [Introduction to PySpark](https://app.datacamp.com/learn/courses/introduction-to-pyspark). The course has the following chapters:\n",
    "\n",
    "1. **Basics: Getting to know PySpark**: The current notebook.\n",
    "2. Manipulating data\n",
    "3. Getting started with machine learning pipelines\n",
    "\n",
    "**Table of contents**:\n",
    "\n",
    "- [1.1 Setup](#1.1-Setup)\n",
    "- [1.2 Creating a Spark Session](#1.2-Creating-a-Spark-Session)\n",
    "- [1.3 Spark SQL Dataframes: Uploading and Consulting](#1.3-Spark-SQL-Dataframes:-Uploading-and-Consulting)\n",
    "- [1.4 Common Methods and Attributes of the SQL Dataframe](#1.4-Common-Methods-and-Attributes-of-the-SQL-Dataframe)\n",
    "- [1.5 SQL Queries](#1.5-SQL-Queries)\n",
    "- [1.6 Pandafying: Convert a Spark SQL Dataframe into a Pandas Dataframe](#1.6-Pandafying:-Convert-a-Spark-SQL-Dataframe-into-a-Pandas-Dataframe)\n",
    "- [1.7 Sparkifying: Convert a Pandas Dataframe into a Spark SQL Dataframe](#1.7-Sparkifying:-Convert-a-Pandas-Dataframe-into-a-Spark-SQL-Dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd5c2c-22a4-40b8-9fce-1cc8685f2418",
   "metadata": {},
   "source": [
    "## 1.1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1303a71-1c80-43d8-a1d2-120fda8f4292",
   "metadata": {},
   "source": [
    "### Install and Run PySpark\n",
    "\n",
    "To install PySpark locally:\n",
    "\n",
    "```bash\n",
    "conda activate ds # select an environment\n",
    "pip install pyspark\n",
    "pip install findspark\n",
    "```\n",
    "\n",
    "We can launch a Spark session in the Terminal locally as follows:\n",
    "\n",
    "```bash\n",
    "conda activate ds\n",
    "pyspark\n",
    "# SparkContext available as 'sc'\n",
    "# Web UI at: http://localhost:4040/\n",
    "# SparkSession available as 'spark'\n",
    "\n",
    "sc # <SparkContext master=local[*] appName=PySparkShell>\n",
    "sc.version # '3.4.0'\n",
    "\n",
    "# Now, we execute the scripts we want, which are using the sc SparkContext\n",
    "\n",
    "```\n",
    "\n",
    "An example script can be:\n",
    "\n",
    "```python\n",
    "import random\n",
    "num_samples = 100000000\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "# Note that sc is not imported, but it's already available\n",
    "# This will execute the function inside() with spark\n",
    "# It might take some time if run locally, because Spark is optimized\n",
    "# for large and distributed datasets\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "\n",
    "print(pi)\n",
    "sc.stop()\n",
    "```\n",
    "\n",
    "### Running on a Notebook\n",
    "\n",
    "If we want to use PySpark on a Jupyter notebook, we need to change the environment variables in `~/.bashrc` or `~/.zshrc`:\n",
    "\n",
    "```bash\n",
    "export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS='notebook'\n",
    "```\n",
    "\n",
    "Then, we restart `pyspark` and launch jupyter from it:\n",
    "\n",
    "```bash\n",
    "conda activate ds\n",
    "pyspark\n",
    "jupyter\n",
    "```\n",
    "\n",
    "**Alternatively**, we can use `findspark` without modifying the environment variables and without starting pyspark from outside:\n",
    "\n",
    "```bash\n",
    "conda activate ds\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "Then, the notebook could contain the code as follows:\n",
    "\n",
    "```python\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Pi\") # AppName: Pi\n",
    "num_samples = 100000000\n",
    "\n",
    "def inside(p):     \n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "# This will execute the function inside() with spark\n",
    "# It might take some time if run locally, because Spark is optimized\n",
    "# for large and distributed datasets\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "\n",
    "print(pi) # 3.14185392\n",
    "sc.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c98eab87-90c3-429d-88ef-0f22e1e1ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9d956da-a946-444e-8ed6-3c52086260ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/19 17:27:12 WARN Utils: Your hostname, kasiopeia.local resolves to a loopback address: 127.0.0.1; using 192.168.1.34 instead (on interface en0)\n",
      "23/04/19 17:27:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/19 17:27:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/Users/mxagar/opt/anaconda3/envs/ds/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14165656\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import random\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Pi\") # AppName: Pi\n",
    "num_samples = 100000000\n",
    "\n",
    "def inside(p):     \n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "# This will execute the function inside() with spark\n",
    "# It might take some time if run locally, because Spark is optimized\n",
    "# for large and distributed datasets\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "\n",
    "print(pi) # 3.14185392\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a059385e-d644-4c84-a43b-c086fc8c9dd3",
   "metadata": {},
   "source": [
    "## 1.2 Creating a Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fc8d9b-0756-441a-bda8-fb95a775ca5f",
   "metadata": {},
   "source": [
    "Creating multiple `SparkSession`s and `SparkContext`s can cause issues, use the `SparkSession.builder.getOrCreate()` method instead, which returns an existing `SparkSession` if there's already one in the environment, or creates a new one if necessary. Usually, in a notebook, we run first\n",
    "\n",
    "```python\n",
    "import findspark\n",
    "findspark.init()\n",
    "```\n",
    "\n",
    "And then, we create a session and work with it. If we shut down the notebook/kernel, the session disappears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b71cc321-f8d1-4d84-bdb6-a8b11ef73642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fcc28605350>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mxagar/opt/anaconda3/envs/ds/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or get a (new) SparkSession: session\n",
    "session = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Print session: our SparkSession\n",
    "print(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9534cc5-7b12-4bab-a81a-7ca1cf1c28ef",
   "metadata": {},
   "source": [
    "## 1.3 Spark SQL Dataframes: Uploading and Consulting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2856f5f-ecd2-48b7-98ba-a28a279f7ed5",
   "metadata": {},
   "source": [
    "One of the advantages of Spark is that we can consult the data one the cluster with SQL-like queries. To that end, first we need to upload the data and check that it's accessible. Here, a CSV file is uploaded to Spark and registered as a temporary table view in the `session`. The resulting object we get in the Python environment is a Spark SQL dataframe. In later sections, instead of directly reading from CSV files, Pandas dataframes are converted to Spark SQL dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56582b73-f749-472e-b8cd-acdcbe17a298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Print the tables in the catalog\n",
    "# catalog provides access to all the data inside the cluster\n",
    "# catalog.listTables() lists all tables\n",
    "# Currently, there is no data on the cluster\n",
    "print(session.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41db0cf1-5c37-4482-97ce-fdf306dbe31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file as a DataFrame\n",
    "# We use the SparkSession\n",
    "# We get back flights_df, which is a Spark SQL dataframe.\n",
    "# NOTE: It is also possible to convert a Pandas dataframe into a Spark SQL Dataframe,\n",
    "# shown later\n",
    "flights_df = session.read.csv(\"../data/flights_small.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69fe06c6-1ef2-4df5-8cf8-7a38d5bdf739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a temporary view.\n",
    "# This allows us to query the data using SQL-like syntax in the used session.\n",
    "# Notes:\n",
    "# - The contents in flights_df are not registered in the session catalog, by default!\n",
    "# - flights_df is linked to the session\n",
    "# - We create a temporary view of flights_df named flights in the catalog\n",
    "flights_df.createOrReplaceTempView(\"flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6895564-9d2a-44f6-929f-1c60b23dd641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='flights', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "# Print the list of tables in the SparkSession/catalog\n",
    "print(session.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8e47f75-32ce-493a-aee5-a82779fa0b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flights\n"
     ]
    }
   ],
   "source": [
    "# We can loop the table names\n",
    "for t in session.catalog.listTables():\n",
    "    print(t.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa7fe4f0-a6a1-4070-87ba-6ef84207dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we know the name of a table, we can get its\n",
    "# Spark SQL Dataframe as follows\n",
    "flights_df_ = session.table(\"flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27673412-9c11-4d67-9e32-9dbabb70a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema of original flights_df\n",
    "#flights_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47b43c3d-75be-4504-819a-89e68a7813b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema of second flights_df_: It's the same table\n",
    "#flights_df_.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "889bc464-87cb-4c21-ac64-4a056e877773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Equivalent to .head(2) on flights_df\n",
    "flights_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02f26373-ecf7-4e6c-98c7-2baaf7fbcc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Equivalent to .head(2) on second flights_df_: It's the same table\n",
    "flights_df_.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa407584-91be-43b4-ac95-d588237e7fd6",
   "metadata": {},
   "source": [
    "## 1.4 Common Methods and Attributes of the SQL Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9770cdd0-9272-4a3c-bdaf-44bad252e9c1",
   "metadata": {},
   "source": [
    "You can use several attributes and methods of the `flights_df` DataFrame object to explore the data. Here are some of the most important ones (ficticious column values):\n",
    "\n",
    "- `printSchema()`: This method prints the schema of the DataFrame, which shows the column names and their data types.\n",
    "\n",
    "- `show()`: This method displays the first n rows of the DataFrame in a tabular format. You can specify the number of rows to show using the n parameter (default is 20). For example, `flights_df.show(5)` will show the first 5 rows of the DataFrame.\n",
    "\n",
    "- `head()`: This method returns the first n rows of the DataFrame as a list of Row objects. You can specify the number of rows to return using the n parameter (default is 1).\n",
    "\n",
    "- `count()`: This method returns the number of rows in the DataFrame.\n",
    "\n",
    "- `columns`: This attribute returns a list of the column names in the DataFrame.\n",
    "\n",
    "- `dtypes`: This attribute returns a list of tuples, where each tuple contains the column name and its data type\n",
    "\n",
    "- `describe()`: This method computes summary statistics for the numerical columns in the DataFrame, such as count, mean, standard deviation, minimum, and maximum values.\n",
    "\n",
    "- `select()`: This method allows you to select one or more columns from the DataFrame. For example, `flights_df.select(\"origin\", \"dest\").show()` will display the \"origin\" and \"dest\" columns of the DataFrame.\n",
    "\n",
    "- `filter()`: This method allows you to filter the rows of the DataFrame based on a condition. For example, `flights_df.filter(flights_df[\"delay\"] > 0).show()` will display the rows where the \"delay\" column is greater than 0.\n",
    "\n",
    "- `groupBy()`: This method allows you to group the rows of the DataFrame by one or more columns and perform an aggregation operation, such as sum, count, or average. For example, `flights_df.groupBy(\"origin\").count().show()` will display the number of flights for each origin airport.\n",
    "\n",
    "- `agg()`: This method allows you to perform one or more aggregation operations on the DataFrame. For example, `flights_df.agg({\"delay\": \"mean\", \"distance\": \"max\"}).show()` will display the mean delay and maximum distance of all flights in the DataFrame.\n",
    "\n",
    "- `join()`: This method allows you to join two DataFrames based on a common column. For example, `flights_df.join(airports_df, flights_df[\"dest\"] == airports_df[\"iata\"]).show()` will join the \"flights_df\" DataFrame with the \"airports_df\" DataFrame on the \"dest\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a438e325-fe23-4eeb-80ed-78a9c7fb077b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, month: int, day: int, dep_time: string, dep_delay: string, arr_time: string, arr_delay: string, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: string, distance: int, hour: string, minute: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We get the schema of the df we uploaded\n",
    "flights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c027150-28a4-4e36-aa5d-b2bd01f61ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(year=2014, month=12, day=8, dep_time='658', dep_delay='-7', arr_time='935', arr_delay='-5', carrier='VX', tailnum='N846VA', flight=1780, origin='SEA', dest='LAX', air_time='132', distance=954, hour='6', minute='58')\n"
     ]
    }
   ],
   "source": [
    "# We get the first row of the df we uploaded\n",
    "print(flights_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12bf4011-b408-44c9-b486-f0be8571ce02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Equivalent to pd.head()\n",
    "flights_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f7a6175-23c7-4e6b-a996-d1cfa693a46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows\n",
    "flights_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee7f85c2-aeaf-49e3-ab47-25660891bed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'dep_time',\n",
       " 'dep_delay',\n",
       " 'arr_time',\n",
       " 'arr_delay',\n",
       " 'carrier',\n",
       " 'tailnum',\n",
       " 'flight',\n",
       " 'origin',\n",
       " 'dest',\n",
       " 'air_time',\n",
       " 'distance',\n",
       " 'hour',\n",
       " 'minute']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54da32f4-b92a-4881-9b42-98df1424f099",
   "metadata": {},
   "source": [
    "## 1.5 SQL Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99770e96-9a73-4768-a54f-3819f1d3e709",
   "metadata": {},
   "source": [
    "One of the big advantages of Spark is that we can access the data on the cluster using SQL-like queries! We get as response a Spark SQL dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6424f10b-1e00-4d0b-a65e-cf555b22980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query\n",
    "query = \"FROM flights SELECT * LIMIT 10\"\n",
    "\n",
    "# Get the first 10 rows of flights\n",
    "# The returning object is a Spark SQL dataframe, as flights_df\n",
    "# but this time it contains only 10 rows\n",
    "flights10 = session.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec3147bf-caf8-4ed6-8bf7-15d8a47dc10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
      "|2014|    1| 15|    1037|        7|    1352|        2|     WN| N646SW|    48|   PDX| DEN|     121|     991|  10|    37|\n",
      "|2014|    7|  2|     847|       42|    1041|       51|     WN| N422WN|  1520|   PDX| OAK|      90|     543|   8|    47|\n",
      "|2014|    5| 12|    1655|       -5|    1842|      -18|     VX| N361VA|   755|   SEA| SFO|      98|     679|  16|    55|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the results: equivalent to the pandas .head(20)\n",
    "flights10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68628ee5-6a7c-4a27-b125-35473f6ecc46",
   "metadata": {},
   "source": [
    "## 1.6 Pandafying: Convert a Spark SQL Dataframe into a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd11e45-31b4-422e-82ab-bdd2e598d09a",
   "metadata": {},
   "source": [
    "Sometimes, it is more convenient to work **locally** with a Pandas dataframe! A common use-case is when we have computed a table with aggregated values, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d48e767d-f6ba-4b74-a1ab-a4745b85abdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query\n",
    "query = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n",
    "\n",
    "# Run the query\n",
    "flight_counts = session.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bfdd594-68d7-4ca1-8089-904aff8dc909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>dest</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEA</td>\n",
       "      <td>RNO</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SEA</td>\n",
       "      <td>DTW</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SEA</td>\n",
       "      <td>CLE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SEA</td>\n",
       "      <td>LAX</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PDX</td>\n",
       "      <td>SEA</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  origin dest    N\n",
       "0    SEA  RNO    8\n",
       "1    SEA  DTW   98\n",
       "2    SEA  CLE    2\n",
       "3    SEA  LAX  450\n",
       "4    PDX  SEA  144"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the results to a pandas DataFrame\n",
    "pd_counts = flight_counts.toPandas()\n",
    "\n",
    "# Print the head of pd_counts\n",
    "pd_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d55b51-ca4f-4eac-a173-a112199d016b",
   "metadata": {},
   "source": [
    "## 1.7 Sparkifying: Convert a Pandas Dataframe into a Spark SQL Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042c3950-0592-45d2-ac03-b183f5602123",
   "metadata": {},
   "source": [
    "In previous sections, a CSV was directly loaded to Spark andd the resulting Spark SQL Dataframe registered as a temporary view to the catalog of the session.\n",
    "\n",
    "Now, instead of reading CSV tables from Spark, we upload Pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0dfb656-ae63-4b66-826a-0c973e95306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fe2ce3b-fd84-48d2-8ffc-b7520b54042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df = pd.read_csv('../data/airports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbf9a7bf-2a2f-4cf9-9693-75f64dd1f7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faa</th>\n",
       "      <th>name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>alt</th>\n",
       "      <th>tz</th>\n",
       "      <th>dst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04G</td>\n",
       "      <td>Lansdowne Airport</td>\n",
       "      <td>41.130472</td>\n",
       "      <td>-80.619583</td>\n",
       "      <td>1044</td>\n",
       "      <td>-5</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>06A</td>\n",
       "      <td>Moton Field Municipal Airport</td>\n",
       "      <td>32.460572</td>\n",
       "      <td>-85.680028</td>\n",
       "      <td>264</td>\n",
       "      <td>-5</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06C</td>\n",
       "      <td>Schaumburg Regional</td>\n",
       "      <td>41.989341</td>\n",
       "      <td>-88.101243</td>\n",
       "      <td>801</td>\n",
       "      <td>-6</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06N</td>\n",
       "      <td>Randall Airport</td>\n",
       "      <td>41.431912</td>\n",
       "      <td>-74.391561</td>\n",
       "      <td>523</td>\n",
       "      <td>-5</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09J</td>\n",
       "      <td>Jekyll Island Airport</td>\n",
       "      <td>31.074472</td>\n",
       "      <td>-81.427778</td>\n",
       "      <td>11</td>\n",
       "      <td>-4</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   faa                           name        lat        lon   alt  tz dst\n",
       "0  04G              Lansdowne Airport  41.130472 -80.619583  1044  -5   A\n",
       "1  06A  Moton Field Municipal Airport  32.460572 -85.680028   264  -5   A\n",
       "2  06C            Schaumburg Regional  41.989341 -88.101243   801  -6   A\n",
       "3  06N                Randall Airport  41.431912 -74.391561   523  -5   A\n",
       "4  09J          Jekyll Island Airport  31.074472 -81.427778    11  -4   A"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47ac1055-9b5d-482c-afcc-789fc1d78070",
   "metadata": {},
   "outputs": [],
   "source": [
    "planes_df = pd.read_csv('../data/planes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3eb14d5-d230-41e1-9740-b911a15d0b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tailnum</th>\n",
       "      <th>year</th>\n",
       "      <th>type</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>model</th>\n",
       "      <th>engines</th>\n",
       "      <th>seats</th>\n",
       "      <th>speed</th>\n",
       "      <th>engine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N102UW</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>Fixed wing multi engine</td>\n",
       "      <td>AIRBUS INDUSTRIE</td>\n",
       "      <td>A320-214</td>\n",
       "      <td>2</td>\n",
       "      <td>182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Turbo-fan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N103US</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Fixed wing multi engine</td>\n",
       "      <td>AIRBUS INDUSTRIE</td>\n",
       "      <td>A320-214</td>\n",
       "      <td>2</td>\n",
       "      <td>182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Turbo-fan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N104UW</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Fixed wing multi engine</td>\n",
       "      <td>AIRBUS INDUSTRIE</td>\n",
       "      <td>A320-214</td>\n",
       "      <td>2</td>\n",
       "      <td>182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Turbo-fan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N105UW</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Fixed wing multi engine</td>\n",
       "      <td>AIRBUS INDUSTRIE</td>\n",
       "      <td>A320-214</td>\n",
       "      <td>2</td>\n",
       "      <td>182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Turbo-fan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N107US</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Fixed wing multi engine</td>\n",
       "      <td>AIRBUS INDUSTRIE</td>\n",
       "      <td>A320-214</td>\n",
       "      <td>2</td>\n",
       "      <td>182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Turbo-fan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tailnum    year                     type      manufacturer     model  \\\n",
       "0  N102UW  1998.0  Fixed wing multi engine  AIRBUS INDUSTRIE  A320-214   \n",
       "1  N103US  1999.0  Fixed wing multi engine  AIRBUS INDUSTRIE  A320-214   \n",
       "2  N104UW  1999.0  Fixed wing multi engine  AIRBUS INDUSTRIE  A320-214   \n",
       "3  N105UW  1999.0  Fixed wing multi engine  AIRBUS INDUSTRIE  A320-214   \n",
       "4  N107US  1999.0  Fixed wing multi engine  AIRBUS INDUSTRIE  A320-214   \n",
       "\n",
       "   engines  seats  speed     engine  \n",
       "0        2    182    NaN  Turbo-fan  \n",
       "1        2    182    NaN  Turbo-fan  \n",
       "2        2    182    NaN  Turbo-fan  \n",
       "3        2    182    NaN  Turbo-fan  \n",
       "4        2    182    NaN  Turbo-fan  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf479185-ab36-4fce-bf73-405b76b1505a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='flights', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "# Examine the tables in the catalog: only one table - 'flights'\n",
    "print(session.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f700cc4-65a8-4770-b12b-cefe77f13544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark_temp from airports_df\n",
    "airports_sdf = session.createDataFrame(airports_df)\n",
    "# Add airports_sdf to the session's catalog\n",
    "airports_sdf.createOrReplaceTempView(\"airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27619544-1b75-4754-92ac-316ddffa882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark_temp from planes_df\n",
    "planes_sdf = session.createDataFrame(planes_df)\n",
    "# Add planes_sdf to the session's catalog\n",
    "planes_sdf.createOrReplaceTempView(\"planes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfafd2c4-18cf-4fe8-a6a2-c0f0343056a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the tables in the catalog again: now 3 tables - 'flights', 'airports', 'planes'\n",
    "print(session.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c32ca-0687-4227-8d1f-b1ce40ca621a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188548b-c70f-4d37-8083-619f7d20b45e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
