{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c58074-4bd1-48cd-9036-25066893d725",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1303a71-1c80-43d8-a1d2-120fda8f4292",
   "metadata": {},
   "source": [
    "### Install and Run PySpark\n",
    "\n",
    "To install PySpark locally:\n",
    "\n",
    "```bash\n",
    "conda activate ds # select an environment\n",
    "pip install pyspark\n",
    "pip install findspark\n",
    "```\n",
    "\n",
    "We can launch a Spark session in the Terminal locally as follows:\n",
    "\n",
    "```bash\n",
    "conda activate ds\n",
    "pyspark\n",
    "# SparkContext available as 'sc'\n",
    "# Web UI at: http://localhost:4040/\n",
    "# SparkSession available as 'spark'\n",
    "\n",
    "sc # <SparkContext master=local[*] appName=PySparkShell>\n",
    "sc.version # '3.4.0'\n",
    "\n",
    "# Now, we execute the scripts we want, which are using the sc SparkContext\n",
    "\n",
    "```\n",
    "\n",
    "An example script can be:\n",
    "\n",
    "```python\n",
    "import random\n",
    "num_samples = 100000000\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "# Note that sc is not imported, but it's already available\n",
    "# This will execute the function inside() with spark\n",
    "# It might take some time if run locally, because Spark is optimized\n",
    "# for large and distributed datasets\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "\n",
    "print(pi)\n",
    "sc.stop()\n",
    "```\n",
    "\n",
    "### Running on a Notebook\n",
    "\n",
    "If we want to use PySpark on a Jupyter notebook, we need to change the environment variables in `~/.bashrc` or `~/.zshrc`:\n",
    "\n",
    "```bash\n",
    "export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS='notebook'\n",
    "```\n",
    "\n",
    "Then, we restart `pyspark` and launch jupyter from it:\n",
    "\n",
    "```bash\n",
    "conda activate ds\n",
    "pyspark\n",
    "jupyter\n",
    "```\n",
    "\n",
    "**Alternatively**, we can use `findspark` without modifying the environment variables and without starting pyspark from outside:\n",
    "\n",
    "```bash\n",
    "conda activate ds\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "Then, the notebook could contain the code as follows:\n",
    "\n",
    "```python\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Pi\") # AppName: Pi\n",
    "num_samples = 100000000\n",
    "\n",
    "def inside(p):     \n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "# This will execute the function inside() with spark\n",
    "# It might take some time if run locally, because Spark is optimized\n",
    "# for large and distributed datasets\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "\n",
    "print(pi) # 3.14185392\n",
    "sc.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9d956da-a946-444e-8ed6-3c52086260ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mxagar/opt/anaconda3/envs/ds/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14189612\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Pi\") # AppName: Pi\n",
    "num_samples = 100000000\n",
    "\n",
    "def inside(p):     \n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "# This will execute the function inside() with spark\n",
    "# It might take some time if run locally, because Spark is optimized\n",
    "# for large and distributed datasets\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "\n",
    "print(pi) # 3.14185392\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9842c91d-e22a-48e4-b93a-859b5e522d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
