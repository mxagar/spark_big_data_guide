{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccd93e7f-2dc6-458a-ac8b-e65782beed65",
   "metadata": {},
   "source": [
    "## Coder Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a8dafa-c9ed-4785-98a9-bfa8e2a37edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.ui.port\", 3000).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2270f6d0-65aa-4e74-9e5b-d9d1d8d5089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/sparkify_log_small.json\"\n",
    "logs = spark.read.json(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2e1ca80-867d-4d7c-b19b-517639c15a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e245b67-c804-4d57-8b70-4fcf4b2a0276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we use a function that doesn't exist (e.g., miss-spelled): AttributeError\n",
    "# If we use a column name which doesn't exist: AnalysisException\n",
    "# If the result is too big (to fit in memory): PyJava... OutOfMemoryError\n",
    "# Column-names are case sensitive, unless we set them not to be with spark.conf.set(\"spark.sql.caseSensitive\", \"false\")\n",
    "# If functions are used on the wrong data: PySparkTypeError\n",
    "# If we forget closing a parenthesys: SyntaxError - unexpected EOF parsing\n",
    "log = logs.select([\"userId\",\"firstname\",\"page\",\"song\"]).where(logs.userId==\"1046\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e4484f8-3886-41f4-b4d2-4f50e5678905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: string, firstname: string, page: string, song: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2e1917b-3db4-4fab-9471-518d8b5c42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = logs.where(logs.page == \"NextSong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45f2171d-9033-404c-af9d-c7b8f8533365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66b64871-0964-4fad-b30e-a8e8090b9e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|userId|       sum(length)|\n",
      "+------+------------------+\n",
      "|  2904|         348.57751|\n",
      "|   691|         808.98476|\n",
      "|  2294|13926.819139999998|\n",
      "|  2162|        8289.81315|\n",
      "|  1436|         633.39011|\n",
      "|  2088|3310.0480000000002|\n",
      "|  2275|         1172.1913|\n",
      "|  2756|1076.6344800000002|\n",
      "|   800|         517.17134|\n",
      "|  1394| 5989.630679999999|\n",
      "|   926|1087.8414400000001|\n",
      "|  2696|         200.95955|\n",
      "|   870|         463.51583|\n",
      "|     7| 533.9419499999999|\n",
      "|  1903|        1058.81895|\n",
      "|   591|         219.79383|\n",
      "|   613|         419.26439|\n",
      "|   574|        1286.55491|\n",
      "|   307|         281.28608|\n",
      "|   577|         374.20363|\n",
      "+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs.groupBy(\"userId\").agg(F.sum(songs.length)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e50b918-6f95-48de-b8b2-dac29a00b109",
   "metadata": {},
   "source": [
    "## Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c280fa1-13f0-4637-85d7-fb77b51a0a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "# Create an accumulator (initial value = 0)\n",
    "acc = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd87787d-4724-4973-8e14-6476ad397044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "def process(x):\n",
    "    global acc\n",
    "    if x % 2 == 0:\n",
    "        acc += 1  # Increment accumulator for even numbers\n",
    "    return x * 2\n",
    "\n",
    "rdd.map(process).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9db44182-e4ec-4a54-bf4a-ea92dfbd21d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of even numbers: 2\n"
     ]
    }
   ],
   "source": [
    "# We canâ€™t reliably read the value inside transformations (like map, filter).\n",
    "# Only read .value on the driver after all actions complete.\n",
    "# If a task fails and reruns, the accumulator may be incremented again, causing overcounting.\n",
    "# They are used as logging variables, not suited for conditionals, etc.\n",
    "# NOTE: Accumulators are not inmune to lazy evaluation! We need to .collect() to update them!\n",
    "print(\"Number of even numbers:\", acc.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47003991-dfa9-4d70-8fd4-8d98fd839def",
   "metadata": {},
   "source": [
    "## Broadcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b53ec55-f2f9-4004-b580-03a15fc93c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Use the existing SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "my_dict = {\"item1\": 1, \"item2\": 2, \"item3\": 3, \"item4\": 4} \n",
    "my_list = [\"item1\", \"item2\", \"item3\", \"item4\"]\n",
    "\n",
    "my_dict_bc = sc.broadcast(my_dict)\n",
    "\n",
    "# .value gives access to the broadcasted object.\n",
    "# The broadcasted object is read-only.\n",
    "# Best used for lookups and small-to-medium data (< few 100MB).\n",
    "def my_func(letter):\n",
    "    return my_dict_bc.value[letter] \n",
    "\n",
    "my_list_rdd = sc.parallelize(my_list)\n",
    "\n",
    "result = my_list_rdd.map(lambda x: my_func(x)).collect()\n",
    "\n",
    "print(result)\n",
    "# [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe12eb1e-df83-4f4b-a2c1-16bab81687db",
   "metadata": {},
   "source": [
    "## Spark Web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d077e8e-2ed3-4b90-9200-478f2489afac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
