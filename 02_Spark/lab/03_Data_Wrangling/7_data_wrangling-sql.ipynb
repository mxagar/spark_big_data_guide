{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling with SQL: Spark SQL\n",
    "\n",
    "The PySpark Python API is very nice for pandas users; only, we need to take into account how functional programming works and avoid for loops.\n",
    "\n",
    "PySpark has another way for allowing the users to do exactly the same things as with the Python API: The SQL interface. The SQL interface has these advantages:\n",
    "\n",
    "- We can use exactly the SQL query we'd use in a relational database.\n",
    "- The SQL query is optimized under the hood for better performance.\n",
    "- Using SQL queries avoids needing to learn a new API/library usage.\n",
    "- Any data analyst can very easily user PySpark.\n",
    "\n",
    "> You might prefer SQL over data frames because the syntax is clearer especially for teams already experienced in SQL.\n",
    "\n",
    "> Spark data frames give you more control. You can break down your queries into smaller steps, which can make debugging easier. You can also [cache](https://unraveldata.com/to-cache-or-not-to-cache/) intermediate results or [repartition](https://hackernoon.com/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4) intermediate results.\n",
    "\n",
    "The major difference when using SQL is that we need to register the uploaded dataframe using `df.createOrReplaceTempView(\"tableName\")`. This step creates a temporary view in Spark which allows to query SQL-like statements to analyze the data.\n",
    "\n",
    "Once the table(s) we want have been uploaded and registered, the interface is the following:\n",
    "\n",
    "```python\n",
    "user_log = spark.read.json(\"../data/sparkify_log_small.json\")\n",
    "user_log.createOrReplaceTempView(\"user_log_table\")\n",
    "# We can use any kind of SQL query\n",
    "spark.sql(\"SELECT * FROM user_log_table LIMIT 2\").show() # Get table with first 20 entries\n",
    "spark.sql(\"\"\"\n",
    "          SELECT * \n",
    "          FROM user_log_table\n",
    "          LIMIT 2\"\"\"\n",
    "          ).collect() # Get ALL Rows\n",
    "```\n",
    "\n",
    "Also, note that we can chain several retrieval/analysis functions one after the other. However, these are not executed due to the *lazy evaluation* principle until we `show()` or `collect()` them:\n",
    "\n",
    "- `show()` returns a dataframe with `n` (20, default) entries from the RDD; use for exploration.\n",
    "- `collect()` returns the complete result/table from the RDD in Row elements; use only when needed.\n",
    "\n",
    "Important links with API information:\n",
    "\n",
    "- [Spark SQL built-in functions](https://spark.apache.org/docs/latest/api/sql/index.html)\n",
    "- [Spark SQL guide](https://spark.apache.org/docs/latest/sql-getting-started.html)\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "- [1. Setup](#1.-Setup)\n",
    "- [2. Create a View And Run Queries](#2.-Create-a-View-And-Run-Queries)\n",
    "- [3. User Defined Functions](#3.-User-Defined-Functions)\n",
    "- [4. Converting Results to Pandas](#4.-Converting-Results-to-Pandas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/28 13:18:56 WARN Utils: Your hostname, kasiopeia.local resolves to a loopback address: 127.0.0.1; using 192.168.1.34 instead (on interface en0)\n",
      "23/04/28 13:18:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/28 13:18:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/28 13:18:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "/Users/mxagar/opt/anaconda3/envs/ds/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data wrangling with Spark SQL\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path = \"../data/sparkify_log_small.json\"\n",
    "user_log = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a View And Run Queries\n",
    "\n",
    "The code below creates a temporary view against which you can run SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a temporary view.\n",
    "# This is necessary for SQL data wrangling.\n",
    "# This allows us to query the data using SQL-like syntax in the used session.\n",
    "# Notes:\n",
    "# - The contents in user_log are not registered in the session catalog, by default!\n",
    "# - user_log is linked to the session\n",
    "# - We create a temporary view of user_log named \"user_log_table\" in the catalog\n",
    "user_log.createOrReplaceTempView(\"user_log_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|       artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|Showaddywaddy|Logged In|  Kenneth|     M|          112|Matthews|232.93342| paid|Charlotte-Concord...|   PUT|NextSong|1509380319284|     5132|Christmas Tears W...|   200|1513720872284|\"Mozilla/5.0 (Win...|  1046|\n",
      "|   Lily Allen|Logged In|Elizabeth|     F|            7|   Chase|195.23873| free|Shreveport-Bossie...|   PUT|NextSong|1512718541284|     5027|       Cheryl Tweedy|   200|1513720878284|\"Mozilla/5.0 (Win...|  1000|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Once the table(s) we want have been uploaded and registered,\n",
    "# the interface is .sql()\n",
    "# BUT because of the *lazy evaluation*,\n",
    "# we need to either show() or collect():\n",
    "# - show() returns a dataframe with n (20, default) entries from the RDD; use for exploration\n",
    "# - collect() returns the complete result/table in Row elements; use only when needed\n",
    "spark.sql(\"SELECT * FROM user_log_table LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|       artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|Showaddywaddy|Logged In|  Kenneth|     M|          112|Matthews|232.93342| paid|Charlotte-Concord...|   PUT|NextSong|1509380319284|     5132|Christmas Tears W...|   200|1513720872284|\"Mozilla/5.0 (Win...|  1046|\n",
      "|   Lily Allen|Logged In|Elizabeth|     F|            7|   Chase|195.23873| free|Shreveport-Bossie...|   PUT|NextSong|1512718541284|     5027|       Cheryl Tweedy|   200|1513720878284|\"Mozilla/5.0 (Win...|  1000|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multi-line queries\n",
    "spark.sql('''\n",
    "          SELECT * \n",
    "          FROM user_log_table \n",
    "          LIMIT 2\n",
    "          '''\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   10000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT COUNT(*) \n",
    "          FROM user_log_table \n",
    "          '''\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userID='1046', firstname='Kenneth', page='NextSong', song='Christmas Tears Will Fall'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Be Wary Of A Woman'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Public Enemy No.1'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Reign Of The Tyrants'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Father And Son'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='No. 5'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Seventeen'),\n",
       " Row(userID='1046', firstname='Kenneth', page='Home', song=None),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='War on war'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Killermont Street'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Black & Blue'),\n",
       " Row(userID='1046', firstname='Kenneth', page='Logout', song=None),\n",
       " Row(userID='1046', firstname='Kenneth', page='Home', song=None),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Heads Will Roll'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Bleed It Out [Live At Milton Keynes]'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Clocks'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Love Rain'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song=\"Ry Ry's Song (Album Version)\"),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='The Invisible Man'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Catch You Baby (Steve Pitron & Max Sanna Radio Edit)'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Ask The Mountains'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Given Up (Album Version)'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='El Cuatrero'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Hero/Heroine'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Spring'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Rising Moon'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Tough Little Boys'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song=\"Qu'Est-Ce Que T'Es Belle\"),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Secrets'),\n",
       " Row(userID='1046', firstname='Kenneth', page='NextSong', song='Under The Gun')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT userID, firstname, page, song\n",
    "          FROM user_log_table \n",
    "          WHERE userID == '1046'\n",
    "          '''\n",
    "          ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|            page|\n",
      "+----------------+\n",
      "|           About|\n",
      "|       Downgrade|\n",
      "|           Error|\n",
      "|            Help|\n",
      "|            Home|\n",
      "|           Login|\n",
      "|          Logout|\n",
      "|        NextSong|\n",
      "|   Save Settings|\n",
      "|        Settings|\n",
      "|Submit Downgrade|\n",
      "|  Submit Upgrade|\n",
      "|         Upgrade|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# All unique pages\n",
    "spark.sql('''\n",
    "          SELECT DISTINCT page\n",
    "          FROM user_log_table \n",
    "          ORDER BY page ASC\n",
    "          '''\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also use User-Defined Functions (UDFs)\n",
    "# but we need to register them to we used\n",
    "# as part of the SQL statement\n",
    "spark.udf.register(\"get_hour\",\n",
    "                   lambda x: int(datetime.datetime.fromtimestamp(x / 1000.0).hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Showaddywaddy', auth='Logged In', firstName='Kenneth', gender='M', itemInSession=112, lastName='Matthews', length=232.93342, level='paid', location='Charlotte-Concord-Gastonia, NC-SC', method='PUT', page='NextSong', registration=1509380319284, sessionId=5132, song='Christmas Tears Will Fall', status=200, ts=1513720872284, userAgent='\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='1046', hour='22')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT *, get_hour(ts) AS hour\n",
    "          FROM user_log_table \n",
    "          LIMIT 1\n",
    "          '''\n",
    "          ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL statement with the freshly defined UDF\n",
    "# Note that the statement is not evaluated\n",
    "# due to the *lazy evaluation* principle.\n",
    "# We need to show/collect the query to get the results.\n",
    "songs_in_hour = spark.sql('''\n",
    "          SELECT get_hour(ts) AS hour, COUNT(*) as plays_per_hour\n",
    "          FROM user_log_table\n",
    "          WHERE page = \"NextSong\"\n",
    "          GROUP BY hour\n",
    "          ORDER BY cast(hour as int) ASC\n",
    "          '''\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+\n",
      "|hour|plays_per_hour|\n",
      "+----+--------------+\n",
      "|   0|           375|\n",
      "|   1|           456|\n",
      "|   2|           454|\n",
      "|   3|           382|\n",
      "|   4|           302|\n",
      "|   5|           352|\n",
      "|   6|           276|\n",
      "|   7|           348|\n",
      "|   8|           358|\n",
      "|   9|           375|\n",
      "|  10|           249|\n",
      "|  11|           216|\n",
      "|  12|           228|\n",
      "|  13|           251|\n",
      "|  14|           339|\n",
      "|  15|           462|\n",
      "|  16|           479|\n",
      "|  17|           484|\n",
      "|  18|           430|\n",
      "|  19|           362|\n",
      "+----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "songs_in_hour.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Converting Results to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# The chain of statements/requests\n",
    "# is also executed and the result\n",
    "# transformed into a pd.DataFrame with toPandas()\n",
    "songs_in_hour_pd = songs_in_hour.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hour  plays_per_hour\n",
      "0     0             456\n",
      "1     1             454\n",
      "2     2             382\n",
      "3     3             302\n",
      "4     4             352\n",
      "5     5             276\n",
      "6     6             348\n",
      "7     7             358\n",
      "8     8             375\n",
      "9     9             249\n",
      "10   10             216\n",
      "11   11             228\n",
      "12   12             251\n",
      "13   13             339\n",
      "14   14             462\n",
      "15   15             479\n",
      "16   16             484\n",
      "17   17             430\n",
      "18   18             362\n",
      "19   19             295\n",
      "20   20             257\n",
      "21   21             248\n",
      "22   22             369\n",
      "23   23             375\n"
     ]
    }
   ],
   "source": [
    "print(songs_in_hour_pd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
