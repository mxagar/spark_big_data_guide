{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Programming: Mapping\n",
    "\n",
    "General purpose programming languages are **procedural**: they use for-loops and the like to process data. However, Spark is written in [**Scala**](https://en.wikipedia.org/wiki/Scala_(programming_language)), which is both OOP and **functional**; when using the Python API PySpark, we need to employ the **functional methods** if we want to be fast. Under the hood, the Python code uses [py4j](https://www.py4j.org/) to make calls to the Java Virtual Machine (JVM) where the Scala library is running.\n",
    "\n",
    "Functional programming uses methods like `map()`, `apply()`, `filter()`, etc. In those, we pass a function to the method, which is the applied to the entire dataset, without the need to using for-loops.\n",
    "\n",
    "This **functional programming** style is very well suited for distributed systems and it is related to how MapReduce and Hadoop work.\n",
    "\n",
    "## Pure Functions and Direct Acyclic Graphs (DAGs)\n",
    "\n",
    "So that a function passed to a method such as `map()`, `apply()` or `filter()` works properly:\n",
    "\n",
    "- it should have no side effects on variables outside its scope,\n",
    "- they should not alter the data which is being processed.\n",
    "\n",
    "These functions are called **pure functions**.\n",
    "\n",
    "In Spark, every node makes a copy of the data being processed, so the data is *immutable*. Additionally, the pure functions we apply are usually very simple; we chain them one after the other to define a more complex processing. So a function seems to be composed of multiple subfunctions. All sub-functions need to be pure.\n",
    "\n",
    "The data is not copied for each of the sub-functions; instead, we perform **lazy evaluation**: all sub-functions are chained in **Direct Acyclic Graphs (DAGs)** and they are not run on the data until it is really necessary. The combinations of sub-functions or chained steps before touching any data are called **stages**.\n",
    "\n",
    "This is similar to baking bread: we collect all necessary stuff (ingredients, tools, etc.) and prepare them properly before even starting to make the dough.\n",
    "\n",
    "## Maps\n",
    "\n",
    "In Spark, maps take data as input and then transform that data with whatever function you put in the map. They are like directions for the data telling how each input should get to the output.\n",
    "\n",
    "The first code cell creates a SparkContext object. With the SparkContext, you can input a dataset and parallelize the data across a cluster (since you are currently using Spark in local mode on a single machine, technically the dataset isn't distributed yet).\n",
    "\n",
    "Run the code cell below to instantiate a SparkContext object and then read in the log_of_songs list into Spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/15 18:00:52 WARN Utils: Your hostname, kasiopeia.local resolves to a loopback address: 127.0.0.1; using 192.168.68.116 instead (on interface en0)\n",
      "25/04/15 18:00:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/15 18:00:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "### \n",
    "# You might have noticed this code in the screencast.\n",
    "#\n",
    "# import findspark\n",
    "# findspark.init('spark-2.3.2-bin-hadoop2.7')\n",
    "#\n",
    "# The findspark Python module makes it easier to install\n",
    "# Spark in local mode on your computer. This is convenient\n",
    "# for practicing Spark syntax locally. \n",
    "# However, the workspaces already have Spark installed and you do not\n",
    "# need to use the findspark module\n",
    "#\n",
    "###\n",
    "\n",
    "# Find Spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"maps_and_lazy_evaluation_example\")\n",
    "\n",
    "# Dataset: list of song names\n",
    "log_of_songs = [\n",
    "        \"Despacito\",\n",
    "        \"Nice for what\",\n",
    "        \"No tears left to cry\",\n",
    "        \"Despacito\",\n",
    "        \"Havana\",\n",
    "        \"In my feelings\",\n",
    "        \"Nice for what\",\n",
    "        \"despacito\",\n",
    "        \"All the stars\"\n",
    "]\n",
    "\n",
    "# Parallelize the log_of_songs to use with Spark\n",
    "# sc.parallelize() takes a list and creates an\n",
    "# RDD = Resilient Distributed Dataset, i.e., \n",
    "# a dataset distirbuted across the Spark nodes.\n",
    "# This RDD is represented by distributed_song_log\n",
    "distributed_song_log = sc.parallelize(log_of_songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next code cell defines a function that converts a song title to lowercase. Then there is an example converting the word \"Havana\" to \"havana\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'havana'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_song_to_lowercase(song):\n",
    "    return song.lower()\n",
    "\n",
    "convert_song_to_lowercase(\"Havana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cells demonstrate how to apply this function using a map step. The map step will go through each song in the list and apply the convert_song_to_lowercase() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We map() our function to the RDD\n",
    "# BUT it is not executed, due to the lazy evaluation principle.\n",
    "# We need to run an action, e.g., collect().\n",
    "# With collect() the results from all of the clusters\n",
    "# are taken and gathered into a single list on the master node\n",
    "distributed_song_log.map(convert_song_to_lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that this code cell ran quite quickly. This is because of lazy evaluation. Spark does not actually execute the map step unless it needs to.\n",
    "\n",
    "\"RDD\" in the output refers to resilient distributed dataset. RDDs are exactly what they say they are: fault-tolerant datasets distributed across a cluster. This is how Spark stores data. \n",
    "\n",
    "To get Spark to actually run the map step, you need to use an \"action\". One available action is the collect method. The collect() method takes the results from all of the clusters and \"collects\" them into a single list on the master node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['despacito',\n",
       " 'nice for what',\n",
       " 'no tears left to cry',\n",
       " 'despacito',\n",
       " 'havana',\n",
       " 'in my feelings',\n",
       " 'nice for what',\n",
       " 'despacito',\n",
       " 'all the stars']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With collect() the results from all of the clusters\n",
    "# are taken and gathered into a single list on the master node\n",
    "distributed_song_log.map(convert_song_to_lowercase).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note as well that Spark is not changing the original data set: Spark is merely making a copy. You can see this by running collect() on the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Despacito',\n",
       " 'Nice for what',\n",
       " 'No tears left to cry',\n",
       " 'Despacito',\n",
       " 'Havana',\n",
       " 'In my feelings',\n",
       " 'Nice for what',\n",
       " 'despacito',\n",
       " 'All the stars']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we run collect() without map(), we get\n",
    "# the original immuted data\n",
    "distributed_song_log.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not always have to write a custom function for the map step. You can also use anonymous (lambda) functions as well as built-in Python functions like string.lower(). \n",
    "\n",
    "Anonymous functions are actually a Python feature for writing functional style programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['despacito',\n",
       " 'nice for what',\n",
       " 'no tears left to cry',\n",
       " 'despacito',\n",
       " 'havana',\n",
       " 'in my feelings',\n",
       " 'nice for what',\n",
       " 'despacito',\n",
       " 'all the stars']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usually, the map() functions are defined as lambdas\n",
    "# or anonymoud functions.\n",
    "# Note that we are using the Pythons built-in lower() function\n",
    "# inside Spark!\n",
    "distributed_song_log.map(lambda song: song.lower()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['despacito',\n",
       " 'nice for what',\n",
       " 'no tears left to cry',\n",
       " 'despacito',\n",
       " 'havana',\n",
       " 'in my feelings',\n",
       " 'nice for what',\n",
       " 'despacito',\n",
       " 'all the stars']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_song_log.map(lambda x: x.lower()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
